{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pykonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def parse_clargs():\n",
    "    \"\"\"\n",
    "    Parse and return command-line arguments.\n",
    "    \"\"\"\n",
    "    clargs = Namespace()\n",
    "    clargs.detections_file = \"data/small_test/detections.h5\"\n",
    "    clargs.stations_file = \"data/small_test/stations.h5\"\n",
    "    clargs.traveltime_directory = \"data/small_test/traveltimes\"\n",
    "    clargs.pwave_velocity_file = \"data/small_test/coso_1d_vp.npz\"\n",
    "    clargs.swave_velocity_file = \"data/small_test/coso_1d_vs.npz\"\n",
    "    clargs.output_file = \"output/small_test/results.h5\"\n",
    "    return (clargs)\n",
    "\n",
    "def parse_params():\n",
    "    \"\"\"\n",
    "    Parse and return algorithm parameters.\n",
    "    \"\"\"\n",
    "    params = dict()\n",
    "    params[\"k_max\"] = 10\n",
    "    return (params)\n",
    "\n",
    "    \n",
    "def main():\n",
    "    # Parse command-line arguments.\n",
    "    clargs = parse_clargs()\n",
    "    \n",
    "    # Parse algorithm parameters.\n",
    "    params = parse_params()\n",
    "    \n",
    "    # Load and preprocess input data.\n",
    "    detections = load_detections(clargs.detections_file)\n",
    "    stations = load_stations(clargs.stations_file)\n",
    "    pwave_vmodel = load_velocity_model(clargs.pwave_velocity_file)\n",
    "    swave_vmodel = load_velocity_model(clargs.swave_velocity_file)\n",
    "    \n",
    "    # Compute traveltime lookup tables.\n",
    "    compute_traveltime_lookup_tables(\n",
    "        clargs.traveltime_directory,\n",
    "        detections, \n",
    "        stations, \n",
    "        pwave_vmodel, \n",
    "        swave_vmodel\n",
    "    )\n",
    "    \n",
    "    # Iterate over trial values of k.\n",
    "    for k in range(1, params[\"k_max\"]):\n",
    "    \n",
    "        # Generate initial means.\n",
    "        means = initialize_means(k, detections, stations)\n",
    "        # Initialize convergence flag.\n",
    "        has_converged = False\n",
    "        \n",
    "        # Iteratively update means and associations until convergence.\n",
    "        while not has_converged:\n",
    "            # Update associations\n",
    "            detections = update_associations(detections, means, traveltime_dir)\n",
    "            # Update means\n",
    "            means = update_means(detections, traveltime_dir)\n",
    "        \n",
    "        # Compute the Akaike Information Criterion\n",
    "        aic = aic(detections, means, travltime_dir)\n",
    "        # Store results for post-processing.\n",
    "        write_results(means, aic, clargs.output_file)\n",
    "        \n",
    "    return (True)\n",
    "\n",
    "\n",
    "def aic(detections, means, travltime_dir):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    raise (NotImplementedError(\"aic()\"))\n",
    "    \n",
    "\n",
    "def compute_traveltime_lookup_tables(\n",
    "    traveltime_dir, \n",
    "    detections, \n",
    "    stations, \n",
    "    pwave_vmodel,\n",
    "    swave_vmodel, \n",
    "    overwrite=False,\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute traveltime lookup tables (optionally overwriting existing\n",
    "    tables) for all detections.\n",
    "    \"\"\"\n",
    "    # Make the output directory if it doesn't exist.\n",
    "    os.makedirs(traveltime_dir, exist_ok=True)\n",
    "    # Drop duplicated (network, station, phase) triplets.\n",
    "    detections = detections.drop_duplicates([\"network\", \"station\", \"phase\"])\n",
    "    detections = detections[[\"network\", \"station\", \"phase\"]].values\n",
    "    # Set the index of stations DataFrame to (\"network\", \"station\")\n",
    "    stations = stations.set_index([\"network\", \"station\"])\n",
    "    for network, station, phase in detections:\n",
    "        path = os.path.join(traveltime_dir, f\"{network}.{station}.{phase}.npz\")\n",
    "        if overwrite or not os.path.exists(path):\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"Computing {phase}-wave traveltime lookup table for \"\n",
    "                    f\"{network}.{station}.\"\n",
    "                )\n",
    "            vmodel = pwave_vmodel if phase == \"P\" else swave_vmodel\n",
    "            keys = [\"latitude\", \"longitude\", \"depth\"]\n",
    "            lat, lon, depth = stations.loc[(network, station), keys]\n",
    "            solver = pykonal.solver.PointSourceSolver(coord_sys=\"spherical\")\n",
    "            solver.vv.min_coords = vmodel.min_coords\n",
    "            solver.vv.node_intervals = vmodel.node_intervals\n",
    "            solver.vv.npts = vmodel.npts\n",
    "            solver.vv.values = vmodel.values\n",
    "            solver.src_loc = pykonal.transformations.geo2sph([lat, lon, depth])\n",
    "            solver.solve()\n",
    "            solver.tt.savez(path)\n",
    "\n",
    "\n",
    "def initialize_means(k_max, detections, stations):\n",
    "    \"\"\"\n",
    "    Initialize means for clustering.\n",
    "    \"\"\"\n",
    "    raise (NotImplementedError(\"initialize_means()\"))\n",
    "    \n",
    "    \n",
    "def load_detections(detections_file):\n",
    "    \"\"\"\n",
    "    Read, preprocess, and return detections from file.\n",
    "    \n",
    "    Input file should be a pandas.HDFStore file with a 'detections'\n",
    "    table comprising 'network', 'station', 'phase', and 'time' fields.\n",
    "    \"\"\"\n",
    "    with pd.HDFStore(detections_file, mode=\"r\") as store:\n",
    "        detections = store[\"detections\"]\n",
    "    return (detections)\n",
    "\n",
    "\n",
    "def load_stations(stations_file):\n",
    "    \"\"\"\n",
    "    Read, preprocess, and return stations from file.\n",
    "    \n",
    "    Input file should be a pandas.HDFStore file with a 'stations' table\n",
    "    comprising 'network', 'station', 'latitude', 'longitude', and\n",
    "    'elevation' fields.\n",
    "    \"\"\"\n",
    "    with pd.HDFStore(stations_file, mode=\"r\") as store:\n",
    "        stations = store[\"stations\"]\n",
    "    stations[\"depth\"] = -stations[\"elevation\"]\n",
    "    stations = stations.drop(columns=[\"elevation\"])\n",
    "    return (stations)\n",
    "\n",
    "\n",
    "def load_velocity_model(velocity_file):\n",
    "    \"\"\"\n",
    "    Read velocity file and return as pykonal.fields.ScalarField3D object.\n",
    "    \"\"\"\n",
    "    vmodel = pykonal.fields.load(velocity_file)\n",
    "    return (vmodel)\n",
    "\n",
    "\n",
    "def update_associations(detections, means, traveltime_dir):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    raise (NotImplementedError(\"update_associations()\"))\n",
    "\n",
    "    \n",
    "def update_means(detections, traveltime_dir):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    raise (NotImplementedError(\"update_means()\"))\n",
    "    \n",
    "\n",
    "def write_results(means, aic, output_file):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    raise (NotImplementedError(\"write_results()\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "initialize_means()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-55-dd76438fc836>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Generate initial means.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_means\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;31m# Initialize convergence flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mhas_converged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-dd76438fc836>\u001b[0m in \u001b[0;36minitialize_means\u001b[0;34m(k_max, detections, stations)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mInitialize\u001b[0m \u001b[0mmeans\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mclustering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \"\"\"\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"initialize_means()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: initialize_means()"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37]",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
